{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries and define essential functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import plotly.graph_objects as pgo\n",
    "import plotly.subplots as psub\n",
    "import plotly.express as px\n",
    "from pathlib import Path\n",
    "from plotly.subplots import make_subplots\n",
    "from Utils.plotting_funcs import plotly_nx\n",
    "\n",
    "np.set_printoptions(precision=3, linewidth=120, suppress=True)\n",
    "layout_width = 800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_laplacian(A):\n",
    "    \"\"\"Return the Laplacian matrix, and its eigenvalues and eigenvectors.\"\"\"\n",
    "    D = np.diag(A.sum(1))\n",
    "    L = D - A\n",
    "    lambdas, vectors = np.linalg.eigh(L)\n",
    "    sort = lambdas.argsort()\n",
    "    lambdas = lambdas[sort]\n",
    "    vectors = vectors[:, sort]\n",
    "    return lambdas, vectors, L\n",
    "\n",
    "\n",
    "def get_fiedler(lambdas, vectors, *args):  # *args is here so we can ignore L from get_laplacian\n",
    "    \"\"\"Return the Fiedler vector and its corresponding eigenvalue with multiplicity.\"\"\"\n",
    "    l2 = lambdas[1]\n",
    "    f = vectors[:, 1]\n",
    "    l2_multiplicity = np.count_nonzero(np.isclose(lambdas, l2))\n",
    "    return round(l2, 5), l2_multiplicity, f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graphs(graphs, figsize=14, dotsize=20):\n",
    "    \"\"\"\n",
    "    Utility to plot a lot of graphs from an array of graphs.\n",
    "    Each graphs is a list of edges; each edge is a tuple.\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(figsize, figsize))\n",
    "    fig.patch.set_facecolor(\"white\")  # To make copying possible (no transparent background)\n",
    "    k = int(np.sqrt(len(graphs)))\n",
    "    for i, g in enumerate(graphs):\n",
    "        plt.subplot(k + 1, k + 1, i + 1)\n",
    "        G = nx.from_numpy_array(graphs[g])\n",
    "        nx.draw_kamada_kawai(G, node_size=dotsize)\n",
    "        l2, mul, _ = get_fiedler(*get_laplacian(graphs[g]))\n",
    "        plt.title(f\"Graph {g} - {len(G.edges)}\\nl2={l2} (x{mul})\")\n",
    "        print(\".\", end=\"\")\n",
    "\n",
    "\n",
    "# plot_graphs([[(0,1),(1,2),(1,3)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_thresholds_combs(A, L, lambdas, vectors, func=None, addremove=1, p=False):\n",
    "    \"\"\"Calculate the current lambda_2 and K_lambda_2 and return them.\"\"\"\n",
    "    l2, l2_multiplicity, f = get_fiedler(lambdas, vectors)\n",
    "\n",
    "    K_l2 = 0\n",
    "    s_val = 0\n",
    "    pairs = None\n",
    "    if func is not None:\n",
    "        search = np.empty_like(A)\n",
    "        search[:] = np.nan\n",
    "        for i, j in itertools.combinations(range(len(f)), 2):\n",
    "            if abs(f[i] - f[j]) > 10e-5 and ((addremove == 1 and A[i][j] == 0) or (addremove == -1 and A[i][j] == 1)):\n",
    "                search[i][j] = addremove * (f[i] - f[j]) ** 2\n",
    "\n",
    "            if A[i][j] == 1:\n",
    "                K_l2 = max(K_l2, (f[i] - f[j]) ** 2)\n",
    "\n",
    "        s_val = func(search)\n",
    "        if l2_multiplicity == 1 or addremove == -1:\n",
    "            pairs = {tuple(p) for p in np.argwhere(np.isclose(search, s_val))}\n",
    "        else:\n",
    "            pairs = f\"Unknown, at least {l2_multiplicity} links\"\n",
    "\n",
    "    if p:\n",
    "        print(f\"({l2:.5f}, {K_l2:.5f})\")\n",
    "        print(f\"L  = {np.array2string(L, prefix='L  = ')}\")\n",
    "        print(f\"eval={np.array2string(lambdas)}\")\n",
    "        print(f\"evec={np.array2string(vectors, prefix='evec=')}\")\n",
    "        print(\"-----------------------------------------------\\n\")\n",
    "\n",
    "    return round(K_l2 * 0.2, 5), pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load graphs and plot them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from my_graphs_dataset import GraphDataset\n",
    "\n",
    "NV = 5  # Number of nodes in the graph.\n",
    "\n",
    "selection = {NV: -1}\n",
    "loader = GraphDataset(selection, suppress_output=True)\n",
    "\n",
    "gs = OrderedDict({f\"G{NV},{i}\": nx.to_numpy_array(g) for i, g in enumerate(loader.graphs(raw=False))})\n",
    "\n",
    "print(f\"Drawing {len(gs)} graphs...\")\n",
    "plot_graphs(gs, figsize=30, dotsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use this cell to get MATLAB matrix of a selected graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "G = gs['G5,10']\n",
    "np.savetxt(sys.stdout, G, fmt='%d', delimiter=' ', newline=';\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Possible values of lambda_2 depending on the number of edges\n",
    "If we consider that all edges are unweighted, i.e., they are all equal to 1, lambda_2 will have discrete values between zero and the number of vertices in the graph. The number of these discrete values depends on the number of vertices. Here, we are interested in which values of lambda_2 are possible for a given number of edges and what is the minimum distance between any real value of lambda_2 and the nearest discrete value. We can then calculate an accaptable treshold K_lambda_2 within which the desired value of lambda_2 cannot be achieved with unweighted edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Prepara a dictionary to store the l2 values by number of edges.\n",
    "l2_by_edges = defaultdict(list)\n",
    "\n",
    "# Go through each graph, calculate the Fiedler value, and store it in the dictionary by number of edges.\n",
    "for i, A in enumerate(gs.values()):\n",
    "    l2 = get_fiedler(*get_laplacian(A))[0]\n",
    "    l2_by_edges[np.count_nonzero(A == 1) / 2].append(l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with 3 subplots (rows)\n",
    "fig = make_subplots(rows=3, cols=1,\n",
    "                    subplot_titles=[\n",
    "                        \"Possible λ₂ by number of edges\",\n",
    "                        \"Needed K_λ₂ for specified λ₂\",\n",
    "                        \"Possible values of λ₂\",\n",
    "                    ],\n",
    "                    vertical_spacing=0.1)\n",
    "axis_labels = sorted(l2_by_edges.keys())\n",
    "\n",
    "# 1st subplot - bar chart of possible lambda_2 values by edge count\n",
    "for edge_count in axis_labels:\n",
    "    values = sorted(l2_by_edges[edge_count])\n",
    "    num_values = len(values)\n",
    "\n",
    "    # Plot each lambda_2 value for this edge count\n",
    "    for i, value in enumerate(values):\n",
    "        if value > 0:  # Only plot positive values\n",
    "            fig.add_trace(\n",
    "                pgo.Bar(\n",
    "                    x=[edge_count],\n",
    "                    y=[value],\n",
    "                    text=f\"{value:.3f}\",\n",
    "                    textposition=\"outside\",\n",
    "                    textangle=-90,\n",
    "                    showlegend=False,\n",
    "                    marker_color='blue',\n",
    "                    width=0.8/num_values if num_values > 1 else 0.5,\n",
    "                    offset=(-0.4 + 0.8*i/num_values) if num_values > 1 else 0\n",
    "                ),\n",
    "                row=1, col=1\n",
    "            )\n",
    "\n",
    "# 2nd subplot - line plot for minimum K_lambda_2\n",
    "possible_l2 = sorted(v for k in l2_by_edges for v in l2_by_edges[k] if v > 0)\n",
    "max_l2 = possible_l2[-1]\n",
    "x_line = np.linspace(0, max_l2, 500)\n",
    "y_line = []\n",
    "i = 0\n",
    "for xi in x_line:\n",
    "    # If current xi is greater than the possible_l2, move to the next possible_l2\n",
    "    while i < len(possible_l2)-1 and xi > possible_l2[i]:\n",
    "        i += 1\n",
    "    # Before the first possible_l2, possible_l2[i] must be greater than xi, so we just record the difference.\n",
    "    if i == 0:\n",
    "        y_line.append(possible_l2[i] - xi if i < len(possible_l2) else 0)\n",
    "    # In other cases, we take the minimum of the difference between xi and the two possible_l2 values on left and right.\n",
    "    else:\n",
    "        y_line.append(min(xi - possible_l2[i-1], possible_l2[i] - xi) if i < len(possible_l2) else xi - possible_l2[i-1])\n",
    "\n",
    "fig.add_trace(\n",
    "    pgo.Scatter(\n",
    "        x=x_line,\n",
    "        y=y_line,\n",
    "        mode='lines',\n",
    "        line=dict(color='blue', width=2),\n",
    "        name=\"Min K_lambda_2\"\n",
    "    ),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# 3rd subplot - scatter plot of possible lambda_2 values\n",
    "fig.add_trace(\n",
    "    pgo.Scatter(\n",
    "        x=possible_l2,\n",
    "        y=[1] * len(possible_l2),\n",
    "        mode='markers',\n",
    "        marker=dict(size=10, color='blue'),\n",
    "        name=\"Possible λ₂ values\"\n",
    "    ),\n",
    "    row=3, col=1\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    height=900,\n",
    "    showlegend=False,\n",
    ")\n",
    "\n",
    "# Update axes\n",
    "fig.update_xaxes(title_text=\"Number of edges\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"λ₂\", row=1, col=1)\n",
    "\n",
    "fig.update_xaxes(title_text=\"Reference λ₂\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"Minimum K_λ₂\", row=2, col=1)\n",
    "\n",
    "fig.update_xaxes(title_text=\"λ₂\", row=3, col=1)\n",
    "fig.update_yaxes(showticklabels=False, row=3, col=1)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze lambda_2 sensitivity to changing link weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "Res = namedtuple(\"Res\", [\"pair\", \"best\"])\n",
    "\n",
    "\n",
    "def first_approx_Ghosh(A, l2, f, addremove):\n",
    "    \"\"\"\n",
    "    Implementation of heuristic from\n",
    "\n",
    "    Ghosh, A., Boyd, S., 2006. Growing well-connected graphs, in: Proceedings\n",
    "    of the IEEE Conference on Decision and Control. IEEE., pp. 6605-6611.\n",
    "    https://doi.org/10.1109/CDC.2006.377282\n",
    "\n",
    "    addremove: 1 to add an edge, -1 to remove an edge\n",
    "    \"\"\"\n",
    "    pairs = None\n",
    "    best = None\n",
    "\n",
    "    if l2 > 0:  # We send l2=0 when l2 has multiplicity > 1\n",
    "        search = np.zeros_like(A)\n",
    "        search[:] = np.nan\n",
    "        pairs = {}\n",
    "        # Search through all pairs of nodes (non-existing edges for add, existing edges for remove).\n",
    "        for i, j in itertools.combinations(range(len(f)), 2):\n",
    "            if (addremove == 1 and A[i][j] == 0) or (addremove == -1 and A[i][j] == 1):\n",
    "                approx = (f[i] - f[j]) ** 2\n",
    "                pairs[(i, j)] = round(approx, 2) * addremove\n",
    "                if abs(f[i] - f[j]) > 10e-5:\n",
    "                    search[i][j] = approx * addremove\n",
    "\n",
    "        # Find the pair of nodes that maximizes the approximation.\n",
    "        s_val = np.nanmax(search)\n",
    "        best = {tuple(p) for p in np.argwhere(np.isclose(search, s_val))}\n",
    "\n",
    "    # if p:\n",
    "    #     print(f\"({l2:.5f}, {K_l2:.5f})\")\n",
    "    #     print(f\"L  = {np.array2string(L, prefix='L  = ')}\")\n",
    "    #     print(f\"eval={np.array2string(lambdas)}\")\n",
    "    #     print(f\"evec={np.array2string(vectors, prefix='evec=')}\")\n",
    "    #     print('-----------------------------------------------\\n')\n",
    "\n",
    "    return Res(pairs, best)\n",
    "\n",
    "\n",
    "def second_approx_He(A, addremove):\n",
    "    \"\"\"\n",
    "    Implementation of second-order heuristic (13) from\n",
    "\n",
    "    He, Z., 2019. Optimization of convergence rate via algebraic connectivity.\n",
    "\n",
    "    addremove: 1 to add an edge, -1 to remove an edge\n",
    "    \"\"\"\n",
    "    n = np.size(A, 1)\n",
    "\n",
    "    D = np.diag(A.sum(1))\n",
    "    Q = D - A\n",
    "\n",
    "    eps = 0.5 / np.max(D)\n",
    "\n",
    "    R = np.eye(n) - eps * Q - 1 / n * np.ones(n)\n",
    "\n",
    "    lambdas, vectors = np.linalg.eigh(R)\n",
    "    sort = lambdas.argsort()\n",
    "    lambdas = lambdas[sort]\n",
    "    vectors = vectors[:, sort]\n",
    "    l1 = lambdas[-1]\n",
    "    z = vectors[:, -1]\n",
    "\n",
    "    search = np.empty_like(A)\n",
    "    search[:] = np.nan\n",
    "    pairs = {}\n",
    "    # Search through all pairs of nodes (non-existing edges for add, existing edges for remove).\n",
    "    for i, j in itertools.combinations(range(n), 2):\n",
    "        # Create a perturbed adjacency matrix.\n",
    "        if (addremove == 1 and A[i][j] == 0) or (addremove == -1 and A[i][j] == 1):\n",
    "            dA = np.zeros_like(A)\n",
    "            dA[i, j] = addremove\n",
    "            dA[j, i] = addremove\n",
    "        else:\n",
    "            continue\n",
    "        # Calculate the approximation and store it.\n",
    "        dQ = np.diag(dA.sum(1)) - dA\n",
    "        approx = z.T @ dQ @ z - (eps / l1) * z.T @ np.linalg.matrix_power(dQ, 2) @ z\n",
    "        pairs[(i, j)] = round(approx, 2)\n",
    "        if abs(approx) > 10e-5:\n",
    "            search[i, j] = approx\n",
    "\n",
    "    # Find the pair of nodes that maximizes the approximation.\n",
    "    s_val = np.nanmax(search)\n",
    "    best = {tuple(p) for p in np.argwhere(np.isclose(search, s_val))}\n",
    "\n",
    "    return Res(pairs, best)\n",
    "\n",
    "\n",
    "def exact_He(A, addremove):\n",
    "    \"\"\"\n",
    "    Implementation of the exact formulation (8) from\n",
    "\n",
    "    He, Z., 2019. Optimization of convergence rate via algebraic connectivity.\n",
    "\n",
    "    addremove: 1 to add an edge, -1 to remove an edge\n",
    "    \"\"\"\n",
    "    n = np.size(A, 1)\n",
    "\n",
    "    Q = np.diag(A.sum(1)) - A\n",
    "\n",
    "    lambdas, vectors = np.linalg.eigh(Q)\n",
    "    sort = lambdas.argsort()\n",
    "    lambdas = lambdas[sort]\n",
    "    vectors = vectors[:, sort]\n",
    "    lambdas = np.insert(lambdas, 0, -1)  # Insert an extra element so we can use 1-indexing\n",
    "    vectors = np.insert(vectors, 0, -1, axis=1)\n",
    "\n",
    "    sum_range = set(range(1, n + 1)) - {2}\n",
    "    search = np.empty_like(A)\n",
    "    search[:] = np.nan\n",
    "    pairs = {}\n",
    "    # Search through all pairs of nodes (non-existing edges for add, existing edges for remove).\n",
    "    for i, j in itertools.combinations(range(n), 2):\n",
    "        # Create a perturbed adjacency matrix.\n",
    "        if (addremove == 1 and A[i][j] == 0) or (addremove == -1 and A[i][j] == 1):\n",
    "            dA = np.zeros_like(A)\n",
    "            dA[i, j] = addremove\n",
    "            dA[j, i] = addremove\n",
    "        else:\n",
    "            continue\n",
    "        # Calculate the approximation and store it.\n",
    "        dQ = np.diag(dA.sum(1)) - dA\n",
    "        approx = vectors[:, 2].T @ dQ @ vectors[:, 2] + sum(\n",
    "            pow(vectors[:, k].T @ dQ @ vectors[:, 2], 2) / (lambdas[2] - lambdas[k]) for k in sum_range\n",
    "        )\n",
    "        pairs[(i, j)] = round(approx, 2)\n",
    "        if abs(approx) > 10e-5:\n",
    "            search[i, j] = approx\n",
    "\n",
    "    # Find the pair of nodes that maximizes the approximation.\n",
    "    s_val = np.nanmax(search)\n",
    "    best = {tuple(p) for p in np.argwhere(np.isclose(search, s_val))}\n",
    "\n",
    "    errors = None\n",
    "    # for p in best:\n",
    "    #     dA = np.zeros_like(A)\n",
    "    #     dA[p[0], p[1]] = addremove\n",
    "    #     dA[p[1], p[0]] = addremove\n",
    "    #     dQ = np.diag(dA.sum(1)) - dA\n",
    "    #     errors[p] = np.linalg.norm(dQ, 'fro') ** 3\n",
    "\n",
    "    return Res(pairs, best), errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The individual plots below show:\n",
    "- graph on the left sides\n",
    "- sensitivity of lambda_2 to changing link weights on the right side\n",
    "- starting value of lambda_2 and corresponding Fiedler vector in the title\n",
    "- below: the predicted value of change in lambda_2 when adding or removing a link using all three methods\n",
    "  - the biggest (adding) or the smallest non-zero (removing) values are bolded\n",
    "\n",
    "Solid lines represent links that are being added.  \n",
    "Dashed lines represent links that are being removed.  \n",
    "Markers (square, circle, x) represent the predicted value of new lambda_2 when adding or removing the best link using the three methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "\n",
    "\n",
    "def analyze_sensitiviy_and_approximations(k, A_start, p=False):\n",
    "    x = np.linspace(0, 1, 50)  # Discrete values to change the link weights.\n",
    "    N = A_start.shape[0]       # Number of nodes in the graph.\n",
    "\n",
    "    r_proto = {1: Res(None, None), -1: Res(None, None)}\n",
    "    e_proto = {1: None, -1: None}\n",
    "    symbols = {\"1st\": \"square\", \"2nd\": \"circle\", \"exact\": \"x\"}\n",
    "    results = {\"1st\": copy(r_proto), \"2nd\": copy(r_proto), \"exact\": copy(r_proto)}\n",
    "    errors = {\"1st\": copy(e_proto), \"2nd\": copy(e_proto), \"exact\": copy(e_proto)}\n",
    "\n",
    "    # Initial lambda value and max change estimate according to Ghosh.\n",
    "    l2_0, lambda_mul, f = get_fiedler(*get_laplacian(A_start))\n",
    "    l2 = l2_0 if lambda_mul == 1 else 0\n",
    "    results[\"1st\"][1] = first_approx_Ghosh(A_start, l2, f, addremove=1)\n",
    "    results[\"1st\"][-1] = first_approx_Ghosh(A_start, l2, f, addremove=-1)\n",
    "\n",
    "    # If lambda2 is unique, calculate other estimates as well.\n",
    "    if lambda_mul == 1:\n",
    "        results[\"2nd\"][1] = second_approx_He(A_start, addremove=1)\n",
    "        results[\"2nd\"][-1] = second_approx_He(A_start, addremove=-1)\n",
    "\n",
    "        results[\"exact\"][1], errors[\"exact\"][1] = exact_He(A_start, addremove=1)\n",
    "        results[\"exact\"][-1], errors[\"exact\"][-1] = exact_He(A_start, addremove=-1)\n",
    "    # Otherwise, calculate node centralities.\n",
    "    else:\n",
    "        G = nx.from_numpy_array(A_start)\n",
    "        centrality = nx.closeness_centrality(G)\n",
    "\n",
    "    # Numerically calculate real lambda2 changes.\n",
    "    sensitivity_data = {}\n",
    "    trend = {}\n",
    "    for i in range(0, N):\n",
    "        for j in range(i + 1, N):\n",
    "            # Make a copy of the adjacency matrix to avoid modifying the original.\n",
    "            sensitivity_data[(i, j)] = []\n",
    "            A = A_start.copy()\n",
    "\n",
    "            # Calculate continuous changes in l2 when changing the selected link (i, j).\n",
    "            for t in x:\n",
    "                A[i][j] = t\n",
    "                A[j][i] = t\n",
    "                l2, _, _ = get_fiedler(*get_laplacian(A))\n",
    "                sensitivity_data[(i, j)].append(l2)\n",
    "                trend[(i, j)] = \"dash\" if A_start[i][j] else \"solid\"\n",
    "\n",
    "    # Make plots.\n",
    "    fig = psub.make_subplots(rows=1, cols=2)\n",
    "    nodes, edges = plotly_nx(nx.from_numpy_array(A_start))\n",
    "    fig.add_trace(nodes, row=1, col=1)\n",
    "    fig.add_trace(edges, row=1, col=1)\n",
    "\n",
    "    def add_approximation_markers(data, pair_value, err_value, addremove, symbol, color):\n",
    "        if addremove == 1:\n",
    "            x = 1\n",
    "            y = data[0] + pair_value\n",
    "        elif addremove == -1:\n",
    "            x = 0\n",
    "            y = data[-1] + pair_value\n",
    "        else:\n",
    "            return\n",
    "\n",
    "        err = None\n",
    "        if err_value is not None:\n",
    "            err = dict(type=\"data\", array=[err_value])\n",
    "\n",
    "        trace = pgo.Scatter(\n",
    "            x=[x], y=[y], error_y=err, mode=\"markers\", marker=dict(size=8, symbol=symbol, color=color), showlegend=False\n",
    "        )\n",
    "        return trace\n",
    "\n",
    "    mem = set()  # Used to prevent plotting approximation markers multiple times.\n",
    "    col_iter = itertools.cycle(px.colors.qualitative.Plotly)\n",
    "    for edge in sensitivity_data:\n",
    "        # Plot lines for the sensitivity data.\n",
    "        color = next(col_iter)\n",
    "        fig.add_trace(\n",
    "            pgo.Scatter(x=x, y=sensitivity_data[edge], name=str(edge), mode=\"lines\", line=dict(dash=trend[edge], color=color)),\n",
    "            row=1,\n",
    "            col=2,\n",
    "        )\n",
    "\n",
    "        # Plot markers for the approximations.\n",
    "        for method in results:  # 1st approx., 2nd approx., exact formulation\n",
    "            for action in results[method]:  # add or remove link\n",
    "                id = (method, action)\n",
    "                output = results[method][action]\n",
    "                err = errors[method][action]\n",
    "                if id not in mem and output is not None and output.best is not None and edge in output.best:\n",
    "                    if err is None:\n",
    "                        err = {}\n",
    "                    trace = add_approximation_markers(\n",
    "                        sensitivity_data[edge], output.pair[edge], err.get(edge, None), action, symbols[method], color\n",
    "                    )\n",
    "                    fig.add_trace(trace, row=1, col=2)\n",
    "                    mem.add(id)\n",
    "\n",
    "    # Add reference lines showing linear increase/decrease of l2 for a value of 1 (to see if changes are linear).\n",
    "    fig.add_trace(\n",
    "        pgo.Scatter(x=[0, 1], y=[l2_0, l2_0 + 1], name=\"+ref\", mode=\"lines\", line=dict(dash=\"dot\", color=\"black\")),\n",
    "        row=1,\n",
    "        col=2,\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        pgo.Scatter(x=[0, 1], y=[l2_0 - 1, l2_0], name=\"-ref\", mode=\"lines\", line=dict(dash=\"dot\", color=\"black\")),\n",
    "        row=1,\n",
    "        col=2,\n",
    "    )\n",
    "\n",
    "    # Add common legend entries for all methods.\n",
    "    for method in results:\n",
    "        fig.add_trace(\n",
    "            pgo.Scatter(\n",
    "                x=[0],\n",
    "                y=[0],\n",
    "                name=method,\n",
    "                mode=\"markers\",\n",
    "                visible=\"legendonly\",\n",
    "                marker=dict(size=8, symbol=symbols[method], color=\"black\"),\n",
    "            ),\n",
    "            row=1,\n",
    "            col=2,\n",
    "        )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=dict(text=f\"Graph {k} (l2 = {l2_0}, f = {f})\"),\n",
    "        width=750,\n",
    "        height=600,\n",
    "        margin=dict(l=50, r=50, t=80, b=200),\n",
    "    )\n",
    "\n",
    "    # Print results.\n",
    "    if lambda_mul == 1:\n",
    "        def gen_annot(result):\n",
    "            if result is None:\n",
    "                return \"\"\n",
    "\n",
    "            pairs = result.pair\n",
    "            best = result.best\n",
    "            return \", \".join(\n",
    "                f\"<b>{pair}={val}</b>\" if pair in best else f\"{pair}={val}\" for pair, val in sorted(pairs.items())\n",
    "            )\n",
    "\n",
    "        annot_kwargs = dict(align=\"left\", showarrow=False, xref=\"paper\", yref=\"paper\")\n",
    "        fig.add_annotation(\n",
    "            text=\"<b>Adding links</b><br>\" + \"<br>\".join(gen_annot(results[key][1]) for key in sorted(results)),\n",
    "            x=0.0,\n",
    "            y=-0.3,\n",
    "            **annot_kwargs,\n",
    "        )\n",
    "        fig.add_annotation(\n",
    "            text=\"<b>Removing links</b><br>\" + \"<br>\".join(gen_annot(results[key][-1]) for key in sorted(results)),\n",
    "            x=0.0,\n",
    "            y=-0.55,\n",
    "            **annot_kwargs,\n",
    "        )\n",
    "    else:\n",
    "        annot_kwargs = dict(align=\"left\", showarrow=False, xref=\"paper\", yref=\"paper\")\n",
    "        fig.add_annotation(text=f\"Lambda2 multiplicity is {lambda_mul}.\", x=0.0, y=-0.2, **annot_kwargs)\n",
    "        fig.add_annotation(\n",
    "            text=\"Centralities: <br>\"\n",
    "            + \", \".join(f\"({k})={v:.2f}\" for k, v in sorted(centrality.items(), key=lambda x: x[1])),\n",
    "            x=0.0,\n",
    "            y=-0.4,\n",
    "            **annot_kwargs,\n",
    "        )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "# A = np.array([\n",
    "#     [0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "#     [1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
    "#     [0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "#     [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
    "#     [0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
    "#     [0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
    "#     [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
    "#     [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1],\n",
    "#     [0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "#     [1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "#     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
    "#     [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0],\n",
    "#     [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0],\n",
    "#     [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1],\n",
    "#     [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0],\n",
    "# ], dtype=float\n",
    "# )\n",
    "\n",
    "# gs = {\"A\": A}\n",
    "\n",
    "for k, A_start in enumerate(gs.values()):\n",
    "    analyze_sensitiviy_and_approximations(k, A_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This cell will plot the same graphs as the previous cell, but only for the first (Ghosh) approximation.  \n",
    "- Additionally, it prints which links will be added/remove to maximize/minimize the change in lambda_2 (all four combinations). Then it shows the value of K_lambda_2 threshold for that edge (by default, it is 0.2 times the predicted change in lambda_2). \n",
    "- It also saves the figure in pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HACK: https://github.com/microsoft/vscode-jupyter/issues/8131#issuecomment-1589961116\n",
    "import plotly\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "plotly.offline.init_notebook_mode()\n",
    "display(\n",
    "    HTML(\n",
    "        '<script type=\"text/javascript\" async src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_SVG\"></script>'\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "def analyze_sensitiviy(k, A_start, p=False, save=False):\n",
    "    x = np.linspace(0, 1, 50)\n",
    "    N = A_start.shape[0]\n",
    "\n",
    "    # Initial lambda value and max change estimate.\n",
    "    lambdas, vectors, L = get_laplacian(A_start)\n",
    "    init_lambda, _, f = get_fiedler(lambdas, vectors)\n",
    "\n",
    "    addmax_est, addmax_links = calc_thresholds_combs(A_start, L, lambdas, vectors, np.nanmax, addremove=1, p=False)\n",
    "    addmin_est, addmin_links = calc_thresholds_combs(A_start, L, lambdas, vectors, np.nanmin, addremove=1, p=False)\n",
    "    remmax_est, remmax_links = calc_thresholds_combs(A_start, L, lambdas, vectors, np.nanmax, addremove=-1, p=False)\n",
    "    remmin_est, remmin_links = calc_thresholds_combs(A_start, L, lambdas, vectors, np.nanmin, addremove=-1, p=False)\n",
    "\n",
    "    sens_data = {}\n",
    "    trend = {}\n",
    "    for i in range(0, N):\n",
    "        for j in range(i + 1, N):\n",
    "            sens_data[(i, j)] = []\n",
    "            A = A_start.copy()\n",
    "\n",
    "            # Calculate continuous changes in l2 when changing links.\n",
    "            for t in x:\n",
    "                A[i][j] = t\n",
    "                A[j][i] = t\n",
    "                l2, _, _ = get_fiedler(*get_laplacian(A))\n",
    "                sens_data[(i, j)].append(l2)\n",
    "                trend[(i, j)] = \"dash\" if A_start[i][j] else \"solid\"\n",
    "\n",
    "            # Something else\n",
    "\n",
    "    # Make plots.\n",
    "    fig = psub.make_subplots(rows=1, cols=2)\n",
    "    # Left subplot - graph\n",
    "    nodes, edges = plotly_nx(nx.from_numpy_array(A_start))\n",
    "    fig.add_trace(nodes, row=1, col=1)\n",
    "    fig.add_trace(edges, row=1, col=1)\n",
    "    fig.update_xaxes(showticklabels=False, row=1, col=1)\n",
    "    fig.update_yaxes(showticklabels=False, row=1, col=1)\n",
    "    # Right subplot - sensitivity\n",
    "    for edge in sens_data:\n",
    "        fig.add_trace(\n",
    "            pgo.Scatter(x=x, y=sens_data[edge], name=str(edge), mode=\"lines\", line=dict(dash=trend[edge])), row=1, col=2\n",
    "        )\n",
    "    fig.update_xaxes(title_text=\"$\\large\\mathrm{Edge\\ weight}\\ a_{ij}$\", title_standoff=5, row=1, col=2)\n",
    "    fig.update_yaxes(title_text=\"$\\large\\mathrm{Algebraic\\ connectivity}\\ \\lambda_2$\", title_standoff=5, row=1, col=2)\n",
    "    fig.update_layout(\n",
    "        title=dict(text=f\"Graph {k} (l2 = {init_lambda}, f = {f})\"),\n",
    "        width=900,\n",
    "        height=650,\n",
    "        margin=dict(l=50, r=50, t=80, b=250),\n",
    "        font=dict(size=16),\n",
    "    )\n",
    "\n",
    "    # Print results.\n",
    "    def gen_annot(est, links,):\n",
    "        return (f'Estimated delta = {est}<br>'\n",
    "                f'Links maximizing delta = {links}<br>')\n",
    "\n",
    "    annot_kwargs = dict(align=\"left\", showarrow=False, xref=\"paper\", yref=\"paper\")\n",
    "\n",
    "    fig.add_annotation(\n",
    "        text=\"<b>Adding max link</b><br>\" + gen_annot(addmax_est, addmax_links), x=0.0, y=-0.4, **annot_kwargs\n",
    "    )\n",
    "    fig.add_annotation(\n",
    "        text=\"<b>Adding min link</b><br>\" + gen_annot(addmin_est, addmin_links), x=0.0, y=-0.65, **annot_kwargs\n",
    "    )\n",
    "    fig.add_annotation(\n",
    "        text=\"<b>Removing max link</b><br>\" + gen_annot(remmax_est, remmax_links), x=1, y=-0.4, **annot_kwargs\n",
    "    )\n",
    "    fig.add_annotation(\n",
    "        text=\"<b>Removing min link</b><br>\" + gen_annot(remmin_est, remmin_links), x=1, y=-0.65, **annot_kwargs\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "    if save:\n",
    "        fig.write_image(f\"l2_function_{k}.{save}\")\n",
    "\n",
    "\n",
    "for k, A_start in enumerate(gs.values()):\n",
    "    if k in [3, 7]:\n",
    "        analyze_sensitiviy(k, A_start, save=\"pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Check the consistency of $$\\Delta \\lambda_{2, max} = \\max(f_i - f_j)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "count = Counter()\n",
    "for k, A_start in enumerate(gs.values()):\n",
    "\n",
    "    # Initial lambda value and max change estimate.\n",
    "    lambdas, vectors, L = get_laplacian(A_start)\n",
    "    init_lambda, _, f = get_fiedler(lambdas, vectors)\n",
    "    delta_est, init_links = calc_thresholds_combs(A_start, L, lambdas, vectors, np.nanmax, addremove=1, p=False)\n",
    "\n",
    "    # Go through all pairs of nodes and find the link that maximizes the change in lambda_2.\n",
    "    n = A_start.shape[0]\n",
    "    max_lambda = init_lambda\n",
    "    max_links = set()\n",
    "    for i in range(0, NV):\n",
    "        for j in range(i + 1, NV):\n",
    "            if A_start[i][j] == 0:\n",
    "                A = A_start.copy()\n",
    "                A[i][j] = 1\n",
    "                A[j][i] = 1\n",
    "\n",
    "                new_lambda, _, _ = get_fiedler(*get_laplacian(A))\n",
    "                if (new_lambda - max_lambda) > 10e-5:\n",
    "                    max_lambda = new_lambda\n",
    "                    max_links = {(i, j)}\n",
    "                elif abs(new_lambda - max_lambda) < 10e-5 and abs(new_lambda - init_lambda) > 10e-5:\n",
    "                    max_links.add((i, j))\n",
    "\n",
    "    print(f\"Graph {k}...\", end=\" \")\n",
    "\n",
    "    # Link(s) that maximize the change in lambda_2 are the same as the initial link(s) found by Ghosh.\n",
    "    if max_links == init_links or not max_links or init_links.issubset(max_links):\n",
    "        out = \"OK\"\n",
    "    # Ghosh's estimate includes wrong links, but also the correct one.\n",
    "    elif max_links.issubset(init_links):\n",
    "        out = \"Partial\"\n",
    "    # Approximation failed completely. Analyze why.\n",
    "    else:\n",
    "        out = \"FAIL\"\n",
    "    count[out] += 1\n",
    "\n",
    "    # Print the initial lambda_2, lambda_2 after adding the link, and the estimated and actual links.\n",
    "    print(f\"{out} (l2o={init_lambda}, l2f={max_lambda}, est={init_links}, act={max_links})\", flush=True)\n",
    "\n",
    "    if out == \"FAIL\":\n",
    "        analyze_sensitiviy(k, A_start)\n",
    "\n",
    "total = sum(count.values())\n",
    "print(\"=========================\")\n",
    "print(f\"OK     : {count['OK']}/{total} [{count['OK'] / total * 100:.2f} %]\")\n",
    "print(f\"Partial: {count['Partial']}/{total} [{count['Partial'] / total * 100:.2f} %]\")\n",
    "print(f\"FAIL   : {count['FAIL']}/{total} [{count['FAIL'] / total * 100:.2f} %]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
